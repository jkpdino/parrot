{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a tensor of shape (b, t, e)\n",
    "def self_attention(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Create the w_prime matrix\n",
    "    raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "\n",
    "    # Normalize the weights\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "\n",
    "    return torch.vmm(weights, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_2(x: torch.Tensor) -> torch.Tensor:\n",
    "    #  qi = Wq * xi\n",
    "    #  ki = Wk * xi\n",
    "    #  vi = Wv * xi\n",
    "\n",
    "    # e = ?\n",
    "\n",
    "    # w' = qi^T * kj / sqrt(e)\n",
    "    # w = softmax(w')\n",
    "    # y = w * vi\n",
    "\n",
    "    # \n",
    "\n",
    "    # Create the w_prime matrix\n",
    "    raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "\n",
    "    # Normalize the weights\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "\n",
    "    return torch.bmm(weights, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Headed Self-Attention with Query, Key, and Value\n",
    "\n",
    "Assume we have a e-dimension vector of d-dimension vector embeddings named `x`\n",
    "\n",
    "`x` :: [e, d]\n",
    "\n",
    "We have `h` attention heads. Each attention head will:\n",
    "\n",
    "Use a $W_q$, $W_k$, and $W_v$ weights matrix to calculate Queries, Keys, and Weights.\n",
    "\n",
    "The $W_q$, $W_k$, and $W_v$ weights matrix are of size [d, d], so we get $3d^2$  operations\n",
    "\n",
    "Using `h` attention heads will slow down computation by a factor of `h`, unless we map each vector to a smaller one. We can use a projection from $d \\rightarrow \\frac{d}{4}$ dimensions for the $W_q$, $W_k$, and $W_v$ vectors. We now get q, e, and v vectors with dimensions [d/h].\n",
    "\n",
    "Now, we can run the normal process.\n",
    "\n",
    "$w' = \\frac{q_i^T \\cdot k_j}{\\sqrt{d/h}}$\n",
    "w=softmax(w')\n",
    "y=w*v\n",
    "\n",
    "At the end, we use a e*e matrix to unify the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now a class to implement all this math\n",
    "##\n",
    "## `e`: the number of embeddings dimensions\n",
    "## `heads`: the number of heads to use\n",
    "##\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, e, heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # e must be divisible by the number of heads\n",
    "        assert e % heads == 0\n",
    "\n",
    "        self.e, self.heads = e, heads\n",
    "\n",
    "        # Create the weight matrices\n",
    "        self.tokeys = nn.Linear(e, e, bias=False)\n",
    "        self.toqueries = nn.Linear(e, e, bias=False)\n",
    "        self.tovalues = nn.Linear(e, e, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(e, e, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape (b, t, e)\n",
    "        # b = batch size\n",
    "        # t = sequence length\n",
    "        # e = embedding size\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        queries = self.toqueries(x)\n",
    "        keys    = self.tokeys(x)\n",
    "        values  = self.tovalues(x)\n",
    "\n",
    "        # split the keys, queries, and values into h groups\n",
    "\n",
    "        s = e // h\n",
    "\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n",
    "\n",
    "        # merge the heads into the batch dimension\n",
    "\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        # calculate the raw weights\n",
    "        # (b*h, t, t)\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        # scale the weights\n",
    "        dot = dot / (e ** (1 / 2))\n",
    "\n",
    "        # normalize the weights\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "\n",
    "        # apply the weights to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "\n",
    "        # tranpose the heads back out of the batch dimension\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, e)\n",
    "\n",
    "        # unify the heads\n",
    "        return self.unifyheads(out)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Definition\n",
    "\n",
    "Any architecture designed to process a connected set of units—such as the tokens in a sequence or the pixels in an image—where the only interaction between units is through self-attention.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Wrap the self attention into a repeatable block\n",
    "\n",
    "- Transformer Block\n",
    "  - Self Attention\n",
    "    - Residual connections go around\n",
    "  - Layer Norm\n",
    "  - Feed Forward Layer\n",
    "    - n* MLP\n",
    "    - Residual connections go around\n",
    "  - Layer Norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, e, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "        # Two normalizing layers over the embedding dimension\n",
    "        self.norm1 = nn.LayerNorm(e)\n",
    "        self.norm2 = nn.LayerNorm(e)\n",
    "\n",
    "        # Scale the input to the feed forward layer by a factor of 4\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(e, 4 * e),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * e, e)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
